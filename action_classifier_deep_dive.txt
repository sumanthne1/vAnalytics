Action Classifier Deep Dive
===========================

High-level flow (inputs → outputs)
----------------------------------
Frame (BGR)  
→ Player detector/tracker (`PlayerTracker`) produces per-player bbox + track_id  
→ Pose estimator (`PoseEstimator`) crops bbox (+10% padding), runs MediaPipe Pose → 33 keypoints + visibility, wrapped in `PoseResult`  
→ Action classifier (`ActionClassifier`) turns pose into posture features → rule-based decision tree → `ActionResult(track_id, action, confidence, features, frame_index)`  
→ Segment extractor merges frame-wise actions into temporal `ActionSegment`s → analytics/export

How posture features are built
------------------------------
Source: `volley_analytics/pose/estimator.py` and `volley_analytics/actions/classifier.py`.

Skeleton setup  
- Coordinates: image space, origin top-left, +y points down.  
- Visibility: keypoint.visibility > 0.5.  
- Arm angles: angle at elbows via vectors shoulder–elbow and wrist–elbow.  
- Knee angles: angle at knees via hip–knee and ankle–knee.  
- Hand heights: shoulder.y - wrist.y (positive means hand above shoulder).

Feature extraction path  
```
PoseResult (track_id, skeleton, frame_index)
  └─ read keypoints: shoulders, wrists, elbows, hips, knees, nose
      ├─ left/right hand height = shoulder.y - wrist.y
      ├─ max hand height = max(left, right)
      ├─ elbow angles = skeleton.get_arm_angles()
      ├─ knee angles = skeleton.get_knee_angles(); avg_knee_angle = mean(valid)
      ├─ hands_together = wrist distance < 100 px
      ├─ hands_above_head = any wrist y < nose.y - 30
      ├─ crouched = avg_knee_angle < 140°
      ├─ arms_raised = max_hand_height > 50 px above shoulders
      ├─ arms_symmetry = 1 - |L-R| / max(|L|, |R|, 1)  (1.0 = symmetric)
      ├─ arm_height_diff = |L-R|
      ├─ one_arm_high = xor(hand above ~40 px over shoulder) OR (both above + diff > 50)
      ├─ standing_upright = avg_knee_angle > 150°
      └─ confidence = visible_keypoints / 33
→ PoseFeatures dataclass (used for classification + debugging)
```

Posture states that feed the classifier
---------------------------------------
- `hands_above_head`: at least one wrist 30 px above the nose line.  
- `arms_raised`: any hand >50 px above its shoulder.  
- `hands_together`: wrists within 100 px (platform).  
- `crouched`: knees bent past 140°; `deep_crouch` at 100° used for DIG.  
- `arms_symmetry`: near-1.0 when both hands raised similarly (block/set), low when asymmetric (serve/spike).  
- `one_arm_high`: one clearly raised arm (serve toss/spike wind-up).  
- `standing_upright`: knees ~straight (>150°), separates serve from defensive crouches.  
- Raw metrics: hand heights, elbow angles, knee angles, arm height difference for nuance.

Action decision logic (per frame)
---------------------------------
Inputs: `PoseFeatures`, thresholds (high=50 px, very_high=100 px, crouch=140°, deep=100°, bent_elbow=120°, hands_together=100 px).  
Output: `ActionResult` with optional `secondary_action` hint and per-track temporal smoothing.

Decision flow
```
Start: action=UNKNOWN, conf=0
│
├─ confidence < 0.3 → UNKNOWN
├─ hands_above_head AND max_hand_height > very_high
│    ├─ arms_symmetry > 0.7 → BLOCK (conf 0.8 * symmetry)
│    └─ asymmetric → SERVE if standing_upright else SPIKE (secondary swapped)
├─ one_arm_high AND standing_upright AND not crouched
│    ├─ arm_height_diff > 30 → SERVE
│    └─ else → SET (serve-like lift but symmetric)
├─ crouched AND hands_together
│    ├─ avg_knee_angle < deep_crouch → DIG
│    └─ else → PASS
├─ crouched (only)
│    ├─ arms_raised → REACH
│    └─ else → READY
├─ arms_raised (not above head)
│    ├─ one_arm_high & standing_upright & diff > 40 → SERVE
│    ├─ arms_symmetry > 0.6 & hands high → SET
│    └─ else → REACH
├─ hands_together below shoulders → PASS
└─ otherwise → IDLE (knees straight) or READY (knees bent)
```

Temporal stability and downstream use
-------------------------------------
- Per-track history of last 5 actions; majority over last ≥3 frames overrides current call if a label appears at least twice, smoothing flicker.  
- Each `ActionResult` carries `PoseFeatures` so downstream tools (viz/debug) can explain the posture that triggered the label.  
- Segment extraction groups consecutive frame-level actions per track (minimum 5 frames, small gap tolerance), drops very short/idle/unknown, and maps to common action enums for analytics/export.

Key takeaways for understanding/adjusting postures
--------------------------------------------------
- Posture is purely geometric: heights, angles, distances, and simple symmetry tests derived from visible keypoints. No velocity/temporal derivatives.  
- Crouch vs upright is knee-angle driven; serve vs dig separation hinges on `standing_upright` and `hands_together`.  
- Above-head detection uses the nose as a reference, so camera tilt/head pose can affect BLOCK/SPIKE/SERVE gating.  
- Asymmetry (`one_arm_high`, `arm_height_diff`) is the main signal distinguishing serve/spike from block/set.  
- Confidence gates everything: low keypoint visibility (<30% visible) yields UNKNOWN and prevents segment creation.  
- To refine postures, adjust thresholds or incorporate motion cues (e.g., rising hands over time for spikes, vertical displacement for jumps) before the decision tree.
